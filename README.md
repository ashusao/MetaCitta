# MetaCitta: Deep Meta-Learning for Spatio- Temporal Prediction Across Cities and Tasks

Necessary code for the paper: [MetaCitta: Deep Meta-Learning for Spatio- Temporal Prediction Across Cities and Tasks](https://link.springer.com/chapter/10.1007/978-3-031-33383-5_6)



### Citation
Please cite the following if you use the code:
```
@InProceedings{10.1007/978-3-031-33383-5_6,
author="Sao, Ashutosh
and Gottschalk, Simon
and Tempelmeier, Nicolas
and Demidova, Elena",
editor="Kashima, Hisashi
and Ide, Tsuyoshi
and Peng, Wen-Chih",
title="MetaCitta: Deep Meta-Learning for Spatio-Temporal Prediction Across Cities and Tasks",
booktitle="Advances in Knowledge Discovery and Data Mining",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="70--82",
abstract="Accurate spatio-temporal prediction is essential for capturing city dynamics and planning mobility services. State-of-the-art deep spatio-temporal predictive models depend on rich and representative training data for target regions and tasks. However, the availability of such data is typically limited. Furthermore, existing predictive models fail to utilize cross-correlations across tasks and cities. In this paper, we propose MetaCitta, a novel deep meta-learning approach that addresses the critical challenges of data scarcity and model generalization. MetaCitta adopts the data from different cities and tasks in a generalizable spatio-temporal deep neural network. We propose a novel meta-learning algorithm that minimizes the discrepancy between spatio-temporal representations across tasks and cities. Our experiments with real-world data demonstrate that the proposed MetaCitta approach outperforms state-of-the-art prediction methods for zero-shot learning and pre-training plus fine-tuning. Furthermore, MetaCitta is computationally more efficient than the existing meta-learning approaches.",
isbn="978-3-031-33383-5"
}
```

### References

1. https://torch-two-sample.readthedocs.io/en/latest/
2. https://github.com/BruceBinBoxing/ST-ResNet-Pytorch

